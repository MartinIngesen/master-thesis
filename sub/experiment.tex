\chapter{Experiment}
\label{chap:experiment}


\section{Software Specifications}
\label{sec:softwarespecs}

\begin{itemize}
    \item Ubuntu 18.04.4 LTS Released February 2020.
    \item go version go1.13.3 linux/amd64, released October 2019.
    \item Perl v5.26.1 built for x86\_64-linux-gnu-thread-multi, september 2017 \todo{Is this a problem? Should we perhaps bump this just incase it has anything to say for our results?}
\end{itemize}

\section{Hardware Specifications}
\label{sec:hardwarespecs}
The host system used for running the experiments feature a Intel(R) Core(TM) i7-7600U CPU @ 2.80GHz processor and 24 GB memory. The processor features two physical cores, and is capable of running two threads per core. This means that the processor has a maximum of 4 logical cores.

\section{Datasets}
\label{sec:datasets}

\subsection{Evaluation of existing datasets?}

\subsubsection{Boss of the SOC}
\todo{Describe the BOTS-datasets}
\todo{Run some actual tests against the dataset to see if we get a hit with our rules}

BOTS are datasets created for Splunks Boss of the SOC capture the flag competitions. The datasets are created in a controlled environment, where some adversary actions has been taken. The contestans then have to analyze and hunt in the data to answer several questions.

\textbf{Version 1}
\url{https://github.com/splunk/botsv1}

\todo{Acquire this dataset}

\textbf{Version 2}
\url{https://github.com/splunk/botsv2}

There are two versions; one which only contains attacker-related data, and one with the all the data, including noise. We have chosen to use the full dataset. This version contains a variety of different data sources. The one we are interested in is called xmlwineventlog. The dataset contains 1 093 753 lines of xmlwineventlog data. We have to convert this to our syslog-format, and we can simply do that using a Python script.


\textbf{Version 3}
\url{https://github.com/splunk/botsv3}

Contains 9 212 lines of xmlwineventlog events. We have to convert this to our syslog-format, and we can simply do that using a Python script.

\subsubsection{Mordor}
\url{https://github.com/hunters-forge/mordor}

\subsubsection{EVTX-ATTACK-SAMPLES}
\url{https://github.com/sbousseaden/EVTX-ATTACK-SAMPLES/}



\subsection{Mordor datasets}
The Mordor datasets are pre-recorded events generated by simulating adversarial techniques in a test environment using common red team tools like Empire and Cobalt Strike. The largest dataset in the collection aims to simulate a Advanced Persistent Threat (APT) known as APT3/Gothic Panda. The dataset contains two scenarios, both outlined in MITRE ATT\&CK Evaluation Operational Flow. The flow maps directly to MITREs ATT\&CK framework, by stepping through 10 different "steps".

\begin{enumerate}
\item Initial Compromise
\item Initial Discovery
\item Privilege Escalation
\item Discovery for Lateral Movement
\item Credential Access
\item Lateral Movement
\item Persistence
\item Collection
\item Exfiltration
\item Execution of Persistence
\end{enumerate}

The dataset contains approximately 100 000 log lines, and is made for security analysts to test their skills and tools using real known bad data.

\subsection{High signal, low noise dataset}
The concentrated dataset is a generated dataset that repeats the same 3 log lines that are enough to trigger a rule. The dataset is made in different sizes, from 1 000 lines, to 1 million lines.

This dataset is in no way "realistic", and is only used for performance testing when evaluating our real-time analysis throughput in a "worst case"-scenario setting.

\subsection{Low signal, high noise dataset}
This is the opposite of the highly concentrated dataset which contained the necessary log lines to repeatedly trigger a specific rule. This dataset only contains the necessary events to trigger a single rule once, the rest of the events are simply background noise.

\subsection{Baseline dataset}
This dataset is a dataset with events that are all benign. This dataset is useful for measuring the speed at which the tools process and analyse the events, without triggering any rules.

\subsection{Dataset discussion}

\subsubsection{Are they realistic?}


\subsubsection{Are they representative?}
\textbf{Concentrated dataset}
The concentrated dataset is used strictly for measuring the performance of the systems in a worst-case scenario, and the dataset is in itself not representative of a real world scenario, ingesting log data that doesn't contain malice.

\textbf{Mordor datasets}
The Mordor datasets are not as concentrated and gathered from a test environment using realistic tools, techniques and procedures based on the MITRE ATT\&CK framework.

\section{Dataset processing}
\label{sec:datasetprocessing}

\todo{Tokenization here?}